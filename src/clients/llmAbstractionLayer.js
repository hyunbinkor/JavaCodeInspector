/**
 * LLM ì¶”ìƒí™” ê³„ì¸µ (LLMAbstractionLayer)
 * 
 * Ollamaì™€ vLLM ê°„ì˜ API ì°¨ì´ë¥¼ í¡ìˆ˜í•˜ì—¬ ì¼ê´€ëœ ì¸í„°í˜ì´ìŠ¤ ì œê³µ
 * 
 * ì£¼ìš” ê¸°ëŠ¥:
 * 1. í™˜ê²½ë³„ ìë™ ê°ì§€ (external/internal)
 * 2. ëª¨ë¸ëª… ì •ê·œí™” (Ollama â†” vLLM ë³€í™˜)
 * 3. API íŒŒë¼ë¯¸í„° ë³€í™˜ ë° í‘œì¤€í™”
 * 4. ì‘ë‹µ í˜•ì‹ í†µì¼
 * 5. ì—ëŸ¬ ì²˜ë¦¬ ë° ì¬ì‹œë„ ë¡œì§
 * 
 * ì‚¬ìš© ì˜ˆì‹œ:
 * ```javascript
 * const llmLayer = new LLMAbstractionLayer(config);
 * const response = await llmLayer.generateCompletion(prompt, {
 *   temperature: 0.1,
 *   max_tokens: 2000
 * });
 * ```
 * 
 * @module LLMAbstractionLayer
 */

import { config } from '../config/config.js';
import logger from '../utils/loggerUtils.js';

export class LLMAbstractionLayer {
  /**
   * ìƒì„±ì: LLM ì¶”ìƒí™” ê³„ì¸µ ì´ˆê¸°í™”
   * 
   * @param {Object} customConfig - ì„ íƒì  ì»¤ìŠ¤í…€ ì„¤ì • (í…ŒìŠ¤íŠ¸ìš©)
   */
  constructor(customConfig = null) {
    const cfg = customConfig || config;

    // í™˜ê²½ ê°ì§€ (NODE_ENV ë˜ëŠ” ì„¤ì • ê¸°ë°˜)
    this.environment = process.env.NODE_ENV || cfg.environment || 'external';

    // LLM Provider ê²°ì • (external: ollama, internal: vllm)
    this.provider = this.environment === 'internal' ? 'vllm' :
      (cfg.llm.provider || 'ollama');

    // Providerë³„ ì„¤ì • ë¡œë“œ
    if (this.provider === 'ollama') {
      this.baseURL = cfg.llm.ollama.baseUrl;
      this.model = cfg.llm.ollama.model;
      this.timeout = cfg.llm.ollama.timeout || 180000;
      this.maxRetries = cfg.llm.maxRetries || 3;
    } else if (this.provider === 'vllm') {
      this.baseURL = cfg.llm.vllm.baseUrl;
      this.model = this.normalizeModelName(cfg.llm.vllm.model);
      this.timeout = cfg.llm.vllm.timeout || 180000;
      this.maxRetries = cfg.llm.maxRetries || 3;
    }

    logger.info(`ğŸ”§ LLM ì¶”ìƒí™” ê³„ì¸µ ì´ˆê¸°í™”`);
    logger.info(`  ğŸ“ í™˜ê²½: ${this.environment}`);
    logger.info(`  ğŸ”Œ Provider: ${this.provider}`);
    logger.info(`  ğŸ¤– ëª¨ë¸: ${this.model}`);
    logger.info(`  ğŸ”— ì„œë²„: ${this.baseURL}`);
  }

  /**
   * ëª¨ë¸ëª… ì •ê·œí™” (í™˜ê²½ë³„ ë³€í™˜)
   * 
   * Ollama ëª¨ë¸ëª… â†’ vLLM ëª¨ë¸ëª… ë§¤í•‘
   * 
   * @param {string} model - ì›ë³¸ ëª¨ë¸ëª…
   * @returns {string} ì •ê·œí™”ëœ ëª¨ë¸ëª…
   */
  normalizeModelName(model) {
    if (this.provider === 'vllm') {
      const modelMapping = {
        'qwen3-coder:30b': 'Qwen/Qwen3-Coder-30B-A3B-Instruct-FP8',
        'gpt-oss:120b': 'openai/gpt-oss-120b',
      };

      return modelMapping[model] || model;
    }

    return model;
  }

  /**
   * í†µí•© completion ìƒì„± ì¸í„°í˜ì´ìŠ¤
   * 
   * Providerì— ê´€ê³„ì—†ì´ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ í˜¸ì¶œ ê°€ëŠ¥
   * 
   * @param {string} prompt - ì…ë ¥ í”„ë¡¬í”„íŠ¸
   * @param {Object} options - ìƒì„± ì˜µì…˜
   * @param {number} options.temperature - Temperature (0.0~1.0)
   * @param {number} options.max_tokens - ìµœëŒ€ í† í° ìˆ˜
   * @param {number} options.num_predict - (Ollama í˜¸í™˜) max_tokensì˜ ë³„ì¹­
   * @returns {Promise<string>} LLM ì‘ë‹µ í…ìŠ¤íŠ¸
   */
  async generateCompletion(prompt, options = {}) {
    const params = this.buildRequestParams(prompt, options);

    let lastError = null;
    for (let attempt = 1; attempt <= this.maxRetries; attempt++) {
      try {
        logger.info(`ğŸ”„ LLM í˜¸ì¶œ ì‹œë„ ${attempt}/${this.maxRetries}`);

        if (this.provider === 'ollama') {
          return await this.callOllama(params);
        } else if (this.provider === 'vllm') {
          return await this.callVLLM(params);
        } else {
          throw new Error(`ì§€ì›í•˜ì§€ ì•ŠëŠ” Provider: ${this.provider}`);
        }
      } catch (error) {
        lastError = error;
        logger.warn(`âš ï¸ LLM í˜¸ì¶œ ì‹¤íŒ¨ (${attempt}/${this.maxRetries}): ${error.message}`);

        if (attempt < this.maxRetries) {
          const delay = Math.min(1000 * Math.pow(2, attempt), 10000);
          logger.info(`   â³ ${delay}ms í›„ ì¬ì‹œë„...`);
          await this.sleep(delay);
        }
      }
    }

    throw new Error(`LLM í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨: ${lastError.message}`);
  }

  /**
   * ìš”ì²­ íŒŒë¼ë¯¸í„° ë¹Œë“œ (Providerë³„ ë³€í™˜)
   * 
   * í†µì¼ëœ ì˜µì…˜ì„ Providerë³„ API í˜•ì‹ìœ¼ë¡œ ë³€í™˜
   * 
   * @param {string} prompt - í”„ë¡¬í”„íŠ¸
   * @param {Object} options - ì˜µì…˜
   * @returns {Object} Providerë³„ ìš”ì²­ íŒŒë¼ë¯¸í„°
   */
  buildRequestParams(prompt, options) {
    // ê³µí†µ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    const temperature = options.temperature ?? 0.1;
    const maxTokens = options.max_tokens || options.num_predict || 2000;

    if (this.provider === 'ollama') {
      return {
        model: this.model,
        prompt: prompt,
        temperature: temperature,
        num_predict: maxTokens,
        stream: false,
        options: {
          top_p: options.top_p || 3,
          repeat_penalty: options.repeat_penalty || 1.1
        }
      };
    } else if (this.provider === 'vllm') {
      // vLLM OpenAI í˜¸í™˜ API í˜•ì‹
      return {
        model: this.model,
        messages: [
          {
            role: "system",
            content: "You are expert in Financial Core System Software Developer."
          },
          {
            role: "user",
            content: prompt
          }
        ],
        temperature: temperature,
        max_tokens: maxTokens,
        top_p: options.top_p || 0.95,
        frequency_penalty: options.frequency_penalty || 0.0,
        presence_penalty: options.presence_penalty || 0.0,
        stop: options.stop || null
      };
    }

    return {};
  }

  /**
   * Ollama API í˜¸ì¶œ
   * 
   * @param {Object} params - ìš”ì²­ íŒŒë¼ë¯¸í„°
   * @returns {Promise<string>} ì‘ë‹µ í…ìŠ¤íŠ¸
   */
  async callOllama(params) {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), this.timeout);

    try {
      const response = await fetch(`${this.baseURL}/api/generate`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(params),
        signal: controller.signal
      });

      if (!response.ok) {
        throw new Error(`Ollama API ì˜¤ë¥˜: ${response.status} ${response.statusText} \n HOST: ${this.baseURL}/api/generate`);
      }

      const data = await response.json();

      // Ollama ì‘ë‹µ í˜•ì‹: { response: "..." }
      return data.response || '';

    } finally {
      clearTimeout(timeoutId);
    }
  }

  /**
   * vLLM API í˜¸ì¶œ (OpenAI í˜¸í™˜ í˜•ì‹)
   * 
   * @param {Object} params - ìš”ì²­ íŒŒë¼ë¯¸í„°
   * @returns {Promise<string>} ì‘ë‹µ í…ìŠ¤íŠ¸
   */
  async callVLLM(params) {
    const controller = new AbortController();
    const timeoutId = setTimeout(() => controller.abort(), this.timeout);

    try {
      const response = await fetch(`${this.baseURL}/v1/chat/completions`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify(params),
        signal: controller.signal
      });

      if (!response.ok) {
        const errorText = await response.text();
        throw new Error(`vLLM API ì˜¤ë¥˜: ${response.status} - ${errorText}`);
      }

      const data = await response.json();

      // vLLM (OpenAI í˜¸í™˜) ì‘ë‹µ í˜•ì‹: { choices: [{ text: "..." }] }
      if (data.choices && data.choices.length > 0) {
        return data.choices[0].message.content || '';
      }

      throw new Error('vLLM ì‘ë‹µì— í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤');

    } finally {
      clearTimeout(timeoutId);
    }
  }

  /**
   * ì—°ê²° ìƒíƒœ í™•ì¸
   * 
   * @returns {Promise<boolean>} ì—°ê²° ì„±ê³µ ì—¬ë¶€
   */
  async checkConnection() {
    try {
      logger.info(`ğŸ” ${this.provider} ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...`);

      const testPrompt = "Hello";
      const response = await this.generateCompletion(testPrompt, {
        temperature: 0.1,
        max_tokens: 100
      });

      if (response && response.length > 0) {
        logger.info(`âœ… ${this.provider} ì—°ê²° ì„±ê³µ`);
        return true;
      }

      throw new Error('ë¹ˆ ì‘ë‹µ');

    } catch (error) {
      logger.error(`âŒ ${this.provider} ì—°ê²° ì‹¤íŒ¨: ${error.message}`);
      throw error;
    }
  }

  /**
   * ë¹„ë™ê¸° sleep ìœ í‹¸ë¦¬í‹°
   * 
   * @param {number} ms - ëŒ€ê¸° ì‹œê°„ (ë°€ë¦¬ì´ˆ)
   * @returns {Promise<void>}
   */
  sleep(ms) {
    return new Promise(resolve => setTimeout(resolve, ms));
  }

  /**
   * JSON ì‘ë‹µ ì •ì œ ë° ì¶”ì¶œ
   * 
   * ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡, íŠ¹ìˆ˜ë¬¸ì ë“±ì„ ì œê±°í•˜ê³  ìˆœìˆ˜ JSONë§Œ ì¶”ì¶œ
   * 
   * @param {string} response - LLM ì‘ë‹µ
   * @returns {Object|null} íŒŒì‹±ëœ JSON ê°ì²´ ë˜ëŠ” null
   */
  cleanAndExtractJSON(response) {
    if (!response || typeof response !== 'string') {
      return null;
    }

    try {
      // 1. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
      let cleaned = response.replace(/```json\s*/g, '').replace(/```\s*/g, '').trim();

      // 2. ì²« { ì°¾ê¸°
      const firstBrace = cleaned.indexOf('{');
      if (firstBrace === -1) {
        return null;
      }

      // 3. ë§ˆì§€ë§‰ } ì°¾ê¸°
      const lastBrace = cleaned.lastIndexOf('}');
      if (lastBrace === -1) {
        return null;
      }

      // 4. JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ
      const jsonStr = cleaned.substring(firstBrace, lastBrace + 1);

      // 5. íŒŒì‹± ì‹œë„
      return JSON.parse(jsonStr);

    } catch (error) {
      logger.warn(`âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨: ${error.message}`);
      return null;
    }
  }
}